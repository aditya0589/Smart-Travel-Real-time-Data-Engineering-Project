# Smart-Travel-Real-time-Data-Engineering-Project
Smart City Real-Time Data Streaming and Analytics
This project implements a complete, end-to-end real-time data streaming pipeline for a smart city environment. It simulates various urban data streams (vehicles, traffic, weather, emergencies), ingests them using Apache Kafka, processes them with Apache Spark Streaming, and persists the data into Amazon S3 for subsequent analysis using AWS services.

Project Goal
The primary objective is to build a robust, scalable data ingestion and processing pipeline capable of handling high-volume, real-time data from diverse IOT devices. The processed, structured data is then stored efficiently in AWS S3 using the Parquet format, preparing it for low-latency querying and business intelligence applications. However, due to the absence of IOT devices, the data used in entirely simulated.

System Architecture
The pipeline follows a data-streaming architecture combining local ingestion/processing with robust cloud storage and analytics.
<img width="1448" height="517" alt="dataengineering-project-architecture drawio" src="https://github.com/user-attachments/assets/00d94b48-1eb7-438a-a83a-c4920b822c78" />


## Technologies Used
**Apache Kafka / ZooKeeper**: 	High-throughput, fault-tolerant message broker for data streams.
**Apache Spark**: PySpark streaming for Real-time stream processing, JSON parsing, schema enforcement, and S3 write operations.
**Docker**:	Docker & Docker Compose	Containerization for easy, reproducible setup of the Spark Cluster and Kafka.
**AWS**: Amazon S3 for Scalable and durable storage for raw and transformed data (Parquet format) and various other services like Amazon Athena and Amazon Redshift for warehousing.


The project handles five distinct data streams, all generated by the data producer script (main.py) and consumed by the streaming processor (spark-city.py).

A. Data Streams Ingested
Data Stream	Kafka Topic	Sample Fields
Vehicle Data	vehicle_data	maker, model, year, fueltype, speed, location
GPS Data	gps_data	speed, direction, vehicleType
Traffic Camera	traffic_data	cameraId, location, snapshot
Weather Data	weather_data	temperature, weatherCondition, windspeed, airQualityIndex
Emergency Data	emergency_data	incident, incidentId, status, description

Export to Sheets
B. Data Processing and S3 Storage
The Spark application (spark-city.py) performs the following critical steps:

Ingestion: Reads raw JSON data from Kafka topics using the internal broker address broker:29092.

Schema Enforcement: Specific StructType schemas are defined and applied to each topic to structure the data.

Watermarking: A 2-minute watermark is applied to the timestamp column for accurate stream processing and managing late data.

S3 Writing: The structured data is written in Parquet format to S3, using a dedicated checkpoint location for fault tolerance.

Local Setup with Docker Compose
These instructions detail how to set up and run the entire pipeline locally using Docker, which is recommended as it handles all networking and dependency issues.

Prerequisites
Docker and Docker Compose installed.

Python 3.x installed (to run the data producer).

1. Project Structure
Ensure your project structure looks like this:

smart-city-streaming/
├── jobs/
│   ├── spark-city.py      # The Spark Streaming application
│   └── config.py          # AWS Credentials
├── main.py                # Data Producer
├── requirements.txt
└── docker-compose.yaml
2. Configure AWS Credentials
The Spark application connects to S3 via the Hadoop S3A connector. You must update the config.py file with your actual AWS keys for the Spark job to successfully write to S3.

Python

# config.py
configuration = {
    # Replace with your actual credentials for S3 connection
    "AWS_ACCESS_KEY": "YOUR_AWS_ACCESS_KEY",
    "AWS_SECRET_KEY": "YOUR_AWS_SECRET_KEY",
}
3. Start the Environment
Build and launch all services (Zookeeper, Kafka Broker, Spark Master, and two Workers) using Docker Compose:

Bash

docker compose up -d
Install the required Python dependencies for the producer script:

Bash

pip install -r requirements.txt
4. Run the Data Producer
Execute the main.py script from your host machine. This connects to Kafka via localhost:9092 (the external listener) and begins generating data:

Bash

python3 main.py
5. Submit the Spark Streaming Job
Submit the spark-city.py application to the Spark Master using your local spark-submit. The --packages flag is crucial for ensuring the Kafka and AWS S3 connectors are available:

Bash

# Execute from your terminal, assuming spark-submit is in your PATH:
spark-submit \
    --master spark://localhost:7077 \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk:1.11.469 \
    ./jobs/spark-city.py
6. Monitoring
Spark UI: Monitor the cluster status and the running stream queries at: http://localhost:9090.

S3: Data will be written continuously to your configured S3 bucket path.

7. Clean Up
Stop and remove all containers when you are finished:

Bash

docker compose down
